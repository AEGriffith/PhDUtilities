{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XYp6UKUHrinH",
        "9tR5ojGwrkB7",
        "lIJR6LMeRlH9"
      ],
      "mount_file_id": "1k3p9vzcZ6ANdSkJijBKQcWFgmwTLg4Mz",
      "authorship_tag": "ABX9TyN51qrzlYU1KWOxzqcqdJwO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AEGriffith/PhDUtilities/blob/main/ACM_Paper_Exctractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ACM Search Information\n",
        "#@markdown Enter your search url:\n",
        "search_url = \"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&ContentItemType=research-article&expand=dl&CCSAnd=60&AfterYear=2018&BeforeYear=2023&AllField=%28Keyword%3A%28Creativity%29+OR+%28Fulltext%3A%28AI%2C+agent%2C+%22Artificial+Intelligence%22%29+AND+Fulltext%3A%28Creativity%29+AND+Fulltext%3A%28Collab*+Support+Tool%29%29%29\" #@param {type: \"string\"}\n",
        "#@markdown Enter the first and last search year:\n",
        "start_year = 2018 #@param {type: \"integer\"}\n",
        "end_year = 2023 #@param {type: \"integer\"}\n",
        "#@markdown Enter the filepath to save csv file (including csv name)\n",
        "filepath = \"/content/drive/MyDrive/Quals/papers.csv\" #@param {type: \"string\"} \n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KD5DuZ6ip7_4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: This code is based on a script that my colleague, Gloria Katuka (https://github.com/gkatuka), wrote. I have adapted it for this specific use case."
      ],
      "metadata": {
        "id": "Y18jGNbM7YlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run All"
      ],
      "metadata": {
        "id": "WBgn7corQRo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "XYp6UKUHrinH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# install chromium, its driver, and selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "hCA7fJRXHhKG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "35mJc4vLoWvK"
      },
      "outputs": [],
      "source": [
        "import urllib3\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
      ],
      "metadata": {
        "id": "A1u5PerdpxQh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "9tR5ojGwrkB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_link(url, year):\n",
        "  \"\"\"\n",
        "    This function modifies a given URL by setting the page size to 50, setting the date range to search\n",
        "    within a given year, and finding the number of pages required to display all search results.\n",
        "\n",
        "    Parameters:\n",
        "    url (str): The URL to be modified.\n",
        "    year (int): The year to set the date range to.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[str, int]: A tuple containing the modified URL, and the number of pages required to display all search results.\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the page size to 50 by replacing any existing pageSize parameter in the URL, or by adding one if it doesn't exist.\n",
        "  if re.findall(r'pageSize=\\d+', url):\n",
        "    url = re.sub(r'pageSize=\\d+', 'pageSize=50', url)\n",
        "  else: \n",
        "    url = url + \"&pageSize=50\"\n",
        "  # Set the date range to search within a given year.\n",
        "  # This is done so that we can search one year at a time, as the ACM limits the number of papers shown to 2000.\n",
        "  url = re.sub(r'AfterYear=\\d+', 'AfterYear={year}', url)\n",
        "  url = re.sub(r'BeforeYear=\\d+', 'BeforeYear={year}', url)\n",
        "  # Find the number of pages by dividing the number of search results by 50 (rounded up).\n",
        "  # To do this the function uses Selenium to get the html source of the page, and then uses BeautifulSoup to parse it.\n",
        "  driver.get(url.format(year=year))\n",
        "  WebDriverWait(driver, 10)\n",
        "  html = driver.page_source\n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "  num_results = int((soup.find(\"span\", {\"class\": \"hitsLength\"}).text).replace(\",\",\"\"))\n",
        "  num_pages: int = (num_results // 50) + 1\n",
        "  # Replace the 'startPage' parameter with the current page number or by adding one if it doesn't exist.\n",
        "  # This will allow the function to navigate through multiple pages of results.\n",
        "  if re.findall(r'startPage=\\d+', url):\n",
        "    url = re.sub(r'startPage=\\d+', 'startPage={page}', url)\n",
        "  else:\n",
        "    url = url + \"&startPage={page}\"\n",
        "  return url, num_pages"
      ],
      "metadata": {
        "id": "Anyh7m8ArmWi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paper_info(search_url, filepath):\n",
        "  \"\"\"\n",
        "    This function extracts information about papers that match a given search query on the ACM Digital Library. \n",
        "    It returns a DataFrame containing the title, DOI, month, year, citation count, and download count for each paper,\n",
        "    and saves it as a csv.\n",
        "\n",
        "    Parameters:\n",
        "    search_url (str): The search URL for the query on the ACM Digital Library\n",
        "\n",
        "    Returns:\n",
        "    df (DataFrame): A DataFrame containing the extracted information for each paper.\n",
        "    \"\"\"\n",
        "\n",
        "  paper_dict = {\"paper_doi\": [], \"paper_title\": [], \"paper_month\": [], \"paper_year\":[], \"citation_count\": [], \"download_count\": []}\n",
        "  paper_urls = []\n",
        "\n",
        "  # Loop through all years in the specified date range\n",
        "  for year in range(start_year, end_year+1):\n",
        "    \n",
        "    page_url, num_pages = modify_link(search_url, year)\n",
        "\n",
        "    # loop through all the pages of the search results for each year\n",
        "    for page in range(num_pages):\n",
        "      driver.get(page_url.format(page=page, year=year))\n",
        "      WebDriverWait(driver, 10)\n",
        "      html = driver.page_source\n",
        "      soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "      # Extract title and doi\n",
        "      title_spans = soup.find_all(\"span\", {\"class\": \"hlFld-Title\"})\n",
        "      for title_span in title_spans:\n",
        "        paper_url = title_span.find(\"a\", href=True)\n",
        "        if paper_url:\n",
        "          paper_url = urllib3.util.url.parse_url(paper_url[\"href\"]).url\n",
        "          paper_urls.append(f'https://dlc.acm.org/{paper_url}')\n",
        "        paper_title = title_span.find(\"a\").text\n",
        "        paper_dict[\"paper_title\"].append(paper_title)\n",
        "        paper_dict[\"paper_doi\"].append(paper_url)\n",
        "\n",
        "      # Extract citation and download counts\n",
        "      metrics = soup.find_all(\"li\", {\"class\": \"metric-holder\"})\n",
        "      for metric in metrics:\n",
        "        # citation count\n",
        "        paper_citation = metric.find(\"div\", {\"class\": \"citation\"})\n",
        "        if paper_citation:\n",
        "          citation = paper_citation.text\n",
        "          citation = citation.replace(\"Total Citations\", \"\")\n",
        "          citation = citation.replace(\",\", \"\")\n",
        "          citation = citation.replace(\" \", \"\")\n",
        "          paper_dict[\"citation_count\"].append(int(citation))\n",
        "        else:\n",
        "          paper_dict[\"citation_count\"].append(0)\n",
        "        # download count\n",
        "        paper_download = metric.find(\"div\", {\"class\": \"metric\"})\n",
        "        if paper_download:\n",
        "          download = paper_download.text\n",
        "          download = download.replace(\"Total Downloads\", \"\")\n",
        "          download = download.replace(\",\", \"\")\n",
        "          download = download.replace(\" \", \"\")\n",
        "          paper_dict[\"download_count\"].append(int(download))\n",
        "        else:\n",
        "          paper_dict[\"download_count\"].append(0)\n",
        "\n",
        "      # Extract paper dates\n",
        "      paper_dates = soup.find_all(\"div\", {\"class\": \"bookPubDate\"})\n",
        "      for date in paper_dates:\n",
        "        date = date.text\n",
        "        # get month, year from date\n",
        "        month, year = date.split(\" \")\n",
        "        paper_dict[\"paper_month\"].append(month)\n",
        "        paper_dict[\"paper_year\"].append(year)\n",
        "  df = pd.DataFrame(paper_dict)\n",
        "  df.to_csv(filepath)\n",
        "  driver.quit()\n",
        "  return df"
      ],
      "metadata": {
        "id": "XEL8O8LDHIC7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "# Get paper lists and information and put it into a dataframe and save as csv.\n",
        "df = get_paper_info(search_url, filepath)\n"
      ],
      "metadata": {
        "id": "FhIOTPlZMnky"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specific processing for my Qualifying Exam"
      ],
      "metadata": {
        "id": "lIJR6LMeRlH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"download_cutoff\"] = False\n",
        "df[\"citation_cutoff\"] = False\n",
        "\n",
        "for year in range(start_year, end_year+1):\n",
        "  df_by_year = df[df['paper_year'].astype(int) == year]\n",
        "  avg_downloads = df_by_year.loc[:, 'download_count'].mean()\n",
        "  df_keep_download = df_by_year[(df_by_year['download_count'] >= avg_downloads)]\n",
        "  avg_citations = df_by_year.loc[:, 'citation_count'].mean()\n",
        "  df_keep_citation = df_by_year[(df_by_year['citation_count'] >= avg_citations)]\n",
        "  df_lose = df_by_year[(df_by_year['download_count'] < avg_downloads)]\n",
        "  df.loc[df_keep_download.index, 'download_cutoff'] = True\n",
        "  df.loc[df_keep_citation.index, 'citation_cutoff'] = True\n",
        "\n",
        "df.to_csv(filepath)"
      ],
      "metadata": {
        "id": "n6lbG9xb3F5h"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count number of papers that meet the criteria for downloads or citations\n",
        "print(\"Number of papers that meet the criteria for downloads: \", df[df['download_cutoff'] == True].shape[0])\n",
        "print(\"Number of papers that meet the criteria for citations: \", df[df['citation_cutoff'] == True].shape[0])\n",
        "\n",
        "# count the number of papers that meet the criteria for downloads but not citations\n",
        "print(\"Number of papers that meet the criteria for downloads but not citations: \", df[(df['download_cutoff'] == True) & (df['citation_cutoff'] == False)].shape[0])\n",
        "\n",
        "# count the number of papers that meet the criteria for citations but not downloads\n",
        "print(\"Number of papers that meet the criteria for citations but not downloads: \", df[(df['download_cutoff'] == False) & (df['citation_cutoff'] == True)].shape[0])\n",
        "\n",
        "# count the number of papers that meet the criteria for both downloads and citations\n",
        "print(\"Number of papers that meet the criteria for both downloads and citations: \", df[(df['download_cutoff'] == True) & (df['citation_cutoff'] == True)].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSwVuNJdmpeY",
        "outputId": "77997b1e-f851-4193-d4a7-44399916de3b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of papers that meet the criteria for downloads:  840\n",
            "Number of papers that meet the criteria for citations:  924\n",
            "Number of papers that meet the criteria for downloads but not citations:  291\n",
            "Number of papers that meet the criteria for citations but not downloads:  375\n",
            "Number of papers that meet the criteria for both downloads and citations:  549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of papers that meet the criteria for both downloads and citations for each year\n",
        "for year in range(start_year, end_year + 1):\n",
        "  print(f\"Total number of papers in {year}: \", df[df['paper_year'].astype(int) == year].shape[0])\n",
        "  print(f\"Number of papers that meet the criteria for both downloads and citations in {year}: \", df[(df['download_cutoff'] == True) & (df['citation_cutoff'] == True) & (df['paper_year'].astype(int) == year)].shape[0])\n",
        "  print(f\"Number of papers that meet the criteria for downloads but not citations in {year}: \", df[(df['download_cutoff'] == True) & (df['citation_cutoff'] == False) & (df['paper_year'].astype(int) == year)].shape[0])\n",
        "  print(f\"Number of papers that meet the criteria for citations but not downloads in {year}: \", df[(df['download_cutoff'] == False) & (df['citation_cutoff'] == True) & (df['paper_year'].astype(int) == year)].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrhHPa9Enq--",
        "outputId": "c62e004a-abf2-41ab-c49a-4ba4504ff650"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of papers in 2018:  432\n",
            "Number of papers that meet the criteria for both downloads and citations in 2018:  97\n",
            "Number of papers that meet the criteria for downloads but not citations in 2018:  26\n",
            "Number of papers that meet the criteria for citations but not downloads in 2018:  53\n",
            "Total number of papers in 2019:  449\n",
            "Number of papers that meet the criteria for both downloads and citations in 2019:  87\n",
            "Number of papers that meet the criteria for downloads but not citations in 2019:  38\n",
            "Number of papers that meet the criteria for citations but not downloads in 2019:  53\n",
            "Total number of papers in 2020:  508\n",
            "Number of papers that meet the criteria for both downloads and citations in 2020:  104\n",
            "Number of papers that meet the criteria for downloads but not citations in 2020:  30\n",
            "Number of papers that meet the criteria for citations but not downloads in 2020:  76\n",
            "Total number of papers in 2021:  624\n",
            "Number of papers that meet the criteria for both downloads and citations in 2021:  128\n",
            "Number of papers that meet the criteria for downloads but not citations in 2021:  74\n",
            "Number of papers that meet the criteria for citations but not downloads in 2021:  103\n",
            "Total number of papers in 2022:  875\n",
            "Number of papers that meet the criteria for both downloads and citations in 2022:  132\n",
            "Number of papers that meet the criteria for downloads but not citations in 2022:  123\n",
            "Number of papers that meet the criteria for citations but not downloads in 2022:  86\n",
            "Total number of papers in 2023:  5\n",
            "Number of papers that meet the criteria for both downloads and citations in 2023:  1\n",
            "Number of papers that meet the criteria for downloads but not citations in 2023:  0\n",
            "Number of papers that meet the criteria for citations but not downloads in 2023:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of papers that meet the criteria for either downloads or citations or both: \", df[\n",
        "    (df['download_cutoff'] == True) | (df['citation_cutoff'] == True)].shape[0])\n",
        "\n",
        "# count the number of papers that meet the criteria for either downloads or citations or both for each year\n",
        "for year in range(start_year, end_year + 1):\n",
        "    print(\"Number of papers that meet the criteria for either downloads or citations or both in {}: \".format(year), df[\n",
        "        ((df['download_cutoff'] == True) | (df['citation_cutoff'] == True)) & (\n",
        "                    df['paper_year'].astype(int) == year)].shape[0])\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVVQyxj6pNCH",
        "outputId": "3b98a205-874e-414f-d3d7-046a9abb38ca"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of papers that meet the criteria for either downloads or citations or both:  840\n",
            "Number of papers that meet the criteria for either downloads or citations or both in 2018:  123\n",
            "Number of papers that meet the criteria for either downloads or citations or both in 2019:  125\n",
            "Number of papers that meet the criteria for either downloads or citations or both in 2020:  134\n",
            "Number of papers that meet the criteria for either downloads or citations or both in 2021:  202\n",
            "Number of papers that meet the criteria for either downloads or citations or both in 2022:  255\n",
            "Number of papers that meet the criteria for either downloads or citations or both in 2023:  1\n"
          ]
        }
      ]
    }
  ]
}