{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1k3p9vzcZ6ANdSkJijBKQcWFgmwTLg4Mz",
      "authorship_tag": "ABX9TyMsW9oM+d6Yum9rLMBY23Mg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AEGriffith/PhDUtilities/blob/main/ACM_Paper_Exctractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ACM Search Information\n",
        "#@markdown Enter your search url:\n",
        "search_url = \"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&ContentItemType=research-article&expand=dl&CCSAnd=60&AfterYear=2018&BeforeYear=2023&AllField=Fulltext%3A%28AI+Agent+%22Artificial+Intelligence%22%29+AND+Fulltext%3A%28Creativity%29+AND+Fulltext%3A%28Collab*+Support+Tool%29\" #@param {type: \"string\"}\n",
        "#@markdown Enter the first and last search year:\n",
        "start_year = 2018 #@param {type: \"integer\"}\n",
        "end_year = 2023 #@param {type: \"integer\"}\n",
        "#@markdown Enter the filepath to save csv file (including csv name)\n",
        "filepath = \"/content/drive/MyDrive/Quals/df_example.csv\" #@param {type: \"string\"} \n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KD5DuZ6ip7_4"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run All"
      ],
      "metadata": {
        "id": "WBgn7corQRo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "XYp6UKUHrinH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# install chromium, its driver, and selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "hCA7fJRXHhKG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "35mJc4vLoWvK"
      },
      "outputs": [],
      "source": [
        "import urllib3\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
      ],
      "metadata": {
        "id": "A1u5PerdpxQh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "9tR5ojGwrkB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_link(url, year):\n",
        "    \"\"\"\n",
        "    Modify the link to increase page size and get number of pages.\n",
        "    Update dates in url to allow for variable dates\n",
        "    \"\"\"\n",
        "    # Set page size to 50\n",
        "    if re.findall(r'pageSize=\\d+', url):\n",
        "      url = re.sub(r'pageSize=\\d+', 'pageSize=50', url)\n",
        "    else: \n",
        "      url = url + \"&pageSize=50\"\n",
        "    # Set dates to equal variable dates of same year (so we can search one year at a time)\n",
        "    url = re.sub(r'AfterYear=\\d+', 'AfterYear={year}', url)\n",
        "    url = re.sub(r'BeforeYear=\\d+', 'BeforeYear={year}', url)\n",
        "    # Finds the number of pages by dividing the number of search results by 50 (rounded up).\n",
        "    driver.get(url.format(year=year))\n",
        "    WebDriverWait(driver, 10)\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    num_results = int((soup.find(\"span\", {\"class\": \"hitsLength\"}).text).replace(\",\",\"\"))\n",
        "    num_pages: int = (num_results // 50) + 1\n",
        "    # replace startPage=0 with startPage={page}\n",
        "    if re.findall(r'startPage=\\d+', url):\n",
        "      url = re.sub(r'startPage=\\d+', 'startPage={page}', url)\n",
        "    else:\n",
        "      url = url + \"&startPage={page}\"\n",
        "    return url, num_pages"
      ],
      "metadata": {
        "id": "Anyh7m8ArmWi"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paper_info(search_url):\n",
        "  \"\"\"\n",
        "  Get the links, titles, citation count, download count, and epub date\n",
        "\n",
        "  Returns a dictionary\n",
        "  \"\"\"\n",
        "\n",
        "  paper_dict = {\"paper_doi\": [], \"paper_title\": [], \"paper_month\": [], \"paper_year\":[], \"citation_count\": [], \"download_count\": []}\n",
        "  paper_urls = []\n",
        "\n",
        "  for year in range(start_year, end_year+1):\n",
        "    \n",
        "    page_url, num_pages = modify_link(search_url, year)\n",
        "\n",
        "    for page in range(num_pages):\n",
        "      driver.get(page_url.format(page=page, year=year))\n",
        "      WebDriverWait(driver, 10)\n",
        "      html = driver.page_source\n",
        "      soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "      # title and doi\n",
        "      title_spans = soup.find_all(\"span\", {\"class\": \"hlFld-Title\"})\n",
        "      print(\"title_spans: \", len(title_spans))\n",
        "      for title_span in title_spans:\n",
        "        paper_url = title_span.find(\"a\", href=True)\n",
        "        if paper_url:\n",
        "          paper_url = urllib3.util.url.parse_url(paper_url[\"href\"]).url\n",
        "          paper_urls.append(f'https://dlc.acm.org/{paper_url}')\n",
        "        paper_title = title_span.find(\"a\").text\n",
        "        paper_dict[\"paper_title\"].append(paper_title)\n",
        "        paper_dict[\"paper_doi\"].append(paper_url)\n",
        "\n",
        "      # citation and download counts\n",
        "      metrics = soup.find_all(\"li\", {\"class\": \"metric-holder\"})\n",
        "      for metric in metrics:\n",
        "        paper_citation = metric.find(\"div\", {\"class\": \"citation\"})\n",
        "        # for citation in paper_citation:\n",
        "        if paper_citation:\n",
        "          citation = paper_citation.text\n",
        "          citation = citation.replace(\"Total Citations\", \"\")\n",
        "          citation = citation.replace(\",\", \"\")\n",
        "          citation = citation.replace(\" \", \"\")\n",
        "          paper_dict[\"citation_count\"].append(int(citation))\n",
        "        else:\n",
        "          paper_dict[\"citation_count\"].append(0)\n",
        "      \n",
        "        paper_download = metric.find(\"div\", {\"class\": \"metric\"})\n",
        "      # print(\"downloads: \", len(paper_downloads))\n",
        "      # for download in paper_downloads:\n",
        "        if paper_download:\n",
        "          download = paper_download.text\n",
        "          download = download.replace(\"Total Downloads\", \"\")\n",
        "          download = download.replace(\",\", \"\")\n",
        "          download = download.replace(\" \", \"\")\n",
        "          paper_dict[\"download_count\"].append(int(download))\n",
        "        else:\n",
        "          paper_dict[\"download_count\"].append(0)\n",
        "\n",
        "      # paper dates\n",
        "      paper_dates = soup.find_all(\"div\", {\"class\": \"bookPubDate\"})\n",
        "      for date in paper_dates:\n",
        "        date = date.text\n",
        "        # get year from date\n",
        "        month, year = date.split(\" \")\n",
        "        paper_dict[\"paper_month\"].append(month)\n",
        "        paper_dict[\"paper_year\"].append(year)\n",
        "  return paper_dict"
      ],
      "metadata": {
        "id": "XEL8O8LDHIC7"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "# Get paper lists and information and put it into a dataframe\n",
        "paper_info = get_paper_info(search_url)\n",
        "df = pd.DataFrame(paper_info)\n",
        "df.to_csv(filepath)\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "FhIOTPlZMnky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specific processing for my Qualifying Exam"
      ],
      "metadata": {
        "id": "lIJR6LMeRlH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kept_papers_count = 0\n",
        "df.to_csv()\n",
        "for year in range(start_year, end_year):\n",
        "  df_by_year = df[df['paper_year'].astype(int)==year]\n",
        "  avg_downloads = df_by_year.loc[:, 'download_count'].mean()\n",
        "  df_keep = df_by_year[(df_by_year['download_count'] >= avg_downloads)]\n",
        "  kept_papers_count += len(df_keep)\n",
        "  df_lose = df_by_year[(df_by_year['download_count'] < avg_downloads)]\n",
        "  df_keep.to_csv(f\"/content/drive/MyDrive/Quals/df_keep_{year}.csv\")\n",
        "  df_lose.to_csv(f\"/content/drive/MyDrive/Quals/df_lose_{year}.csv\")\n",
        "\n",
        "print(kept_papers_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6lbG9xb3F5h",
        "outputId": "d81ccddb-a559-47f8-e7e5-cfa50f202b8d"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "812\n"
          ]
        }
      ]
    }
  ]
}